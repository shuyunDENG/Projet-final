import requests
import pandas as pd
from pathlib import Path
from tqdm import tqdm
from Bio import SeqIO
from Bio.PDB import MMCIFParser
from Bio.PDB.DSSP import DSSP
from io import StringIO
import time
import warnings
warnings.filterwarnings('ignore')

# ==================== ç¬¬1æ­¥: UniProtæœç´¢ ====================
def uniprot_search_ids(query, size=200, reviewed=True):
    """æœç´¢UniProt ID"""
    base = "https://rest.uniprot.org/uniprotkb/search"
    q = query + (" AND reviewed:true" if reviewed else "")
    params = {
        "query": q,
        "fields": "accession",
        "size": size,
        "format": "tsv"
    }

    try:
        r = requests.get(base, params=params, timeout=30)
        r.raise_for_status()
        ids = [line.strip().split("\t")[0]
               for line in r.text.splitlines()[1:] if line.strip()]
        return ids
    except Exception as e:
        print(f"âŒ UniProtæœç´¢å¤±è´¥: {e}")
        return []


def fetch_uniprot_fasta(uniprot_id):
    """è·å–UniProtåºåˆ—"""
    url = f"https://rest.uniprot.org/uniprotkb/{uniprot_id}.fasta"
    try:
        r = requests.get(url, timeout=15)
        r.raise_for_status()
        rec = next(SeqIO.parse(StringIO(r.text), "fasta"))
        return str(rec.seq)
    except Exception as e:
        print(f"âŒ è·å–UniProtåºåˆ—å¤±è´¥: {e}")
        return None
    
# ==================== ç¬¬2æ­¥: RCSBç»“æ„æœç´¢ ====================
def get_pdbs_from_uniprot(uniprot_id):
    """ä»UniProt APIè·å–PDBåˆ—è¡¨ - æœ€å¯é çš„æ–¹æ³•"""
    url = f"https://rest.uniprot.org/uniprotkb/{uniprot_id}.json"

    try:
        r = requests.get(url, timeout=20)
        if r.status_code != 200:
            return []

        data = r.json()
        pdb_list = []

        xrefs = data.get("uniProtKBCrossReferences", [])
        for xref in xrefs:
            if xref.get("database") == "PDB":
                pdb_id = xref.get("id")
                if pdb_id:
                    pdb_list.append(pdb_id)

        return pdb_list
    except Exception as e:
        print(f"âŒ UniProt PDBæœç´¢å¤±è´¥: {e}")
        return []

def rcsb_search_structures(uniprot_id, max_hits=5):
    query = {
        "query": {
            "type": "group",
            "logical_operator": "and",
            "nodes": [
                {
                    "type": "terminal",
                    "service": "text",
                    "parameters": {
                        "attribute": "rcsb_polymer_entity_container_identifiers.reference_sequence_identifiers.database_name",
                        "operator": "exact_match",
                        "value": "UniProt"
                    }
                },
                {
                    "type": "terminal",
                    "service": "text",
                    "parameters": {
                        "attribute": "rcsb_polymer_entity_container_identifiers.reference_sequence_identifiers.database_accession",
                        "operator": "exact_match",
                        "value": uniprot_id
                    }
                }
            ]
        },
        "return_type": "entry",
        "request_options": {"paginate": {"start": 0, "rows": max_hits}}
    }

    try:
        r = requests.post("https://search.rcsb.org/rcsbsearch/v2/query",
                         json=query, timeout=30)
        if r.status_code != 200:
            return []
        result = r.json().get("result_set", [])
        return [x["identifier"] for x in result]
    except Exception as e:
        print(f"âŒ RCSBæœç´¢å¤±è´¥: {e}")
        return []


def get_pdb_resolution(pdb_id):
    """è·å–PDBåˆ†è¾¨ç‡"""
    url = f"https://data.rcsb.org/rest/v1/core/entry/{pdb_id}"
    try:
        r = requests.get(url, timeout=15)
        if r.status_code != 200:
            return None
        data = r.json()
        res_list = data.get("rcsb_entry_info", {}).get("resolution_combined", [])
        return res_list[0] if res_list else None
    except Exception as e:
        print(f"âŒ è·å–PDBåˆ†è¾¨ç‡å¤±è´¥: {e}")
        return None

def choose_best_pdb(uniprot_id):
    """é€‰æ‹©åˆ†è¾¨ç‡æœ€é«˜çš„PDBç»“æ„"""
    pdb_list = rcsb_search_structures(uniprot_id)
    if not pdb_list:
        return None

    best_pdb = None
    best_res = 999.0

    for pdb_id in pdb_list:
        res = get_pdb_resolution(pdb_id)
        if res and res < best_res:
            best_pdb = pdb_id
            best_res = res

    return best_pdb


def choose_best_pdb_with_validation(uniprot_id, max_candidates=20):
    """
    é€‰æ‹©æœ€ä½³PDB - ä¼˜å…ˆç”¨UniProt API, RCSBä½œä¸ºå¤‡é€‰
    """
    print(f"ğŸ” æœç´¢ UniProt {uniprot_id} çš„PDBç»“æ„...")

    # æ–¹æ³•1: UniProt API (ä¼˜å…ˆ)
    pdb_list = get_pdbs_from_uniprot(uniprot_id)

    # æ–¹æ³•2: å¦‚æœå¤±è´¥,å°è¯•RCSB (å¤‡é€‰)
    if not pdb_list:
        print("  âš ï¸  UniProt APIæ— ç»“æœ,å°è¯•RCSB...")
        pdb_list = rcsb_search_structures(uniprot_id, max_hits=max_candidates)

    if not pdb_list:
        print("âŒ ä¸¤ç§æ–¹æ³•éƒ½æœªæ‰¾åˆ°PDBç»“æ„")
        return None, None, None

    # é™åˆ¶æ•°é‡
    pdb_list = pdb_list[:max_candidates]
    print(f"âœ“ æ‰¾åˆ° {len(pdb_list)} ä¸ªå€™é€‰PDB")
    print(f"\nå¼€å§‹éªŒè¯ (å…± {len(pdb_list)} ä¸ª)...")
    print("-" * 60)

    # éªŒè¯å¹¶é€‰æ‹©æœ€ä½³
    valid_pdbs = []

    for idx, pdb_id in enumerate(pdb_list, 1):
        # æ˜¾ç¤ºè¿›åº¦
        print(f"[{idx}/{len(pdb_list)}] æ£€æŸ¥ {pdb_id}...", end=" ")

        resolution = get_pdb_resolution(pdb_id)
        if resolution is None:
            print("æ— åˆ†è¾¨ç‡æ•°æ®")
            continue

        sifts_data = fetch_sifts_mapping(pdb_id)
        mappings = extract_chain_mappings(sifts_data, pdb_id, uniprot_id)

        if mappings:
            coverage = sum(m[2] - m[1] + 1 for m in mappings)
            valid_pdbs.append((pdb_id, resolution, coverage, mappings))
            print(f"âœ“ åˆ†è¾¨ç‡={resolution:.2f}Ã…, è¦†ç›–={coverage}aa, {len(mappings)}é“¾")
        else:
            print("âœ— æ— SIFTSæ˜ å°„")

    print("-" * 60)
    print(f"éªŒè¯å®Œæˆ: {len(valid_pdbs)}/{len(pdb_list)} ä¸ªPDBæœ‰æ•ˆ\n")

    if not valid_pdbs:
        print("âŒ æ‰€æœ‰å€™é€‰PDBéƒ½æ²¡æœ‰æœ‰æ•ˆçš„SIFTSæ˜ å°„!")
        return None, None, None

    # é€‰æ‹©æœ€ä½³
    valid_pdbs.sort(key=lambda x: (x[1], -x[2]))
    best = valid_pdbs[0]

    print(f"ğŸ† æœ€ä½³PDB: {best[0]}")
    print(f"   åˆ†è¾¨ç‡: {best[1]:.2f} Ã…")
    print(f"   è¦†ç›–é•¿åº¦: {best[2]} aa")
    print(f"   é“¾æ•°: {len(best[3])}")

    return best[0], best[1], best[3]

# ==================== ç¬¬3æ­¥: SIFTSæ˜ å°„ ====================
def fetch_sifts_mapping(pdb_id):
    """è·å–PDBåˆ°UniProtçš„æ®‹åŸºæ˜ å°„"""
    url = f"https://www.ebi.ac.uk/pdbe/api/mappings/uniprot_segments/{pdb_id.lower()}"
    try:
        r = requests.get(url, timeout=20)
        if r.status_code != 200:
            return None
        return r.json()
    except Exception as e:
        print(f"âŒ è·å–SIFTSæ˜ å°„å¤±è´¥: {e}")
        return None

def get_uniprot_entry_name(uniprot_id):
    """è·å–UniProtçš„entry name (å¦‚P04637 â†’ P53_HUMAN)"""
    url = f"https://rest.uniprot.org/uniprotkb/{uniprot_id}.json"
    try:
        r = requests.get(url, timeout=15)
        if r.status_code == 200:
            data = r.json()
            return data.get("uniProtkbId")  # è¿™æ˜¯entry name
    except:
        pass
    return None


def extract_chain_mappings(sifts_json, pdb_id, uniprot_id):
    """
    ä»SIFTS JSONæå–é“¾æ˜ å°„ä¿¡æ¯
    æ”¯æŒUniProt ID (P04637) å’Œ Entry Name (P53_HUMAN)
    """
    if not sifts_json:
        return []

    # è·å–entry name
    entry_name = get_uniprot_entry_name(uniprot_id)

    mappings = []
    data = sifts_json.get(pdb_id.lower(), {})

    for uniprot_data in data.get("UniProt", {}).values():
        # æ£€æŸ¥ä¸¤ç§æ ‡è¯†ç¬¦
        identifier = uniprot_data.get("identifier")
        if identifier not in [uniprot_id, entry_name]:
            continue

        for seg in uniprot_data.get("mappings", []):
            chain_id = seg.get("chain_id")
            unp_start = seg.get("unp_start")
            unp_end = seg.get("unp_end")
            pdb_start = seg.get("start", {}).get("residue_number")
            pdb_end = seg.get("end", {}).get("residue_number")

            if all([chain_id, unp_start, unp_end, pdb_start, pdb_end]):
                mappings.append((chain_id, unp_start, unp_end, pdb_start, pdb_end))

    return mappings

# ==================== ç¬¬4æ­¥: DSSPæ ‡æ³¨ ====================
def download_mmcif(pdb_id, cache_dir="pdb_cache"):
    """ä¸‹è½½mmCIFæ–‡ä»¶"""
    Path(cache_dir).mkdir(exist_ok=True)
    cif_path = Path(cache_dir) / f"{pdb_id}.cif"

    if cif_path.exists():
        return cif_path

    urls = [
        f"https://files.rcsb.org/download/{pdb_id}.cif",
        f"https://www.ebi.ac.uk/pdbe/entry-files/download/{pdb_id}.cif"
    ]
    for url in urls:
        try:
           r = requests.get(url, timeout=30)
           if r.status_code == 200:
              cif_path.write_bytes(r.content)
              return cif_path
        except Exception as e:
          print(f"  âš ï¸ ä¸‹è½½{pdb_id}å¤±è´¥: {e}")
    return None


def build_residue_mapping_from_sifts(sifts_json, pdb_id, uniprot_id):
    """
    ä»SIFTS JSONæ„å»ºç²¾ç¡®çš„æ®‹åŸºæ˜ å°„
    è¿”å›: {(chain_id, author_pdb_resseq): uniprot_idx, ...}
    """
    if not sifts_json:
        return {}

    entry_name = get_uniprot_entry_name(uniprot_id)
    residue_map = {}
    data = sifts_json.get(pdb_id.lower(), {})

    for uniprot_data in data.get("UniProt", {}).values():
        identifier = uniprot_data.get("identifier")
        if identifier not in [uniprot_id, entry_name]:
            continue

        for seg in uniprot_data.get("mappings", []):
            chain_id = seg.get("chain_id")
            unp_start = seg.get("unp_start")
            unp_end = seg.get("unp_end")

            pdb_start_info = seg.get("start", {})
            pdb_end_info = seg.get("end", {})
            pdb_start = pdb_start_info.get("residue_number")
            pdb_end = pdb_end_info.get("residue_number")

            if not all([chain_id, unp_start, unp_end, pdb_start, pdb_end]):
                continue

            unp_len = unp_end - unp_start + 1
            pdb_len = pdb_end - pdb_start + 1

            if unp_len != pdb_len:
                print(f"  âš ï¸ è­¦å‘Š: é“¾{chain_id}æ˜ å°„é•¿åº¦ä¸ä¸€è‡´ (UniProt:{unp_len} vs PDB:{pdb_len})")
                map_len = min(unp_len, pdb_len)
            else:
                map_len = unp_len

            for i in range(map_len):
                pdb_resseq = pdb_start + i
                unp_idx = (unp_start - 1) + i
                residue_map[(chain_id, pdb_resseq)] = unp_idx

    return residue_map


def ss8_to_hec(ss8):
    """å°†8æ€äºŒçº§ç»“æ„è½¬æ¢ä¸º3æ€(H/E/C)"""
    if ss8 in ('H', 'G', 'I'):
        return 'H'
    elif ss8 in ('E', 'B'):
        return 'E'
    else:
        return 'C'


def get_residue_numbering_from_mmcif(cif_path, chain_id):
    """
    ä»mmCIFæ–‡ä»¶ä¸­æå–authorç¼–å·åˆ°labelç¼–å·çš„æ˜ å°„
    è¿”å›: {author_seq_id: label_seq_id, ...}
    """
    try:
        parser = MMCIFParser(QUIET=True)
        structure = parser.get_structure('struct', cif_path)

        mmcif_dict = parser._mmcif_dict

        mapping = {}

        if '_atom_site.auth_asym_id' in mmcif_dict:
            auth_chains = mmcif_dict['_atom_site.auth_asym_id']
            auth_seq_ids = mmcif_dict['_atom_site.auth_seq_id']
            label_seq_ids = mmcif_dict['_atom_site.label_seq_id']

            for i in range(len(auth_chains)):
                if auth_chains[i] == chain_id:
                    try:
                        auth_num = int(auth_seq_ids[i])
                        label_num = int(label_seq_ids[i])
                        mapping[auth_num] = label_num
                    except (ValueError, KeyError):
                        continue

        return mapping

    except Exception as e:
        print(f"  âš ï¸ è¯»å–mmCIFç¼–å·æ˜ å°„å¤±è´¥: {e}")
        return {}


def run_dssp(cif_path, chain_id):
    """
    è¿è¡ŒDSSPè·å–äºŒçº§ç»“æ„
    è¿”å›: {label_seq_id: ss_code, ...}
    """
    try:
        parser = MMCIFParser(QUIET=True)
        structure = parser.get_structure(cif_path.stem, cif_path)
        model = structure[0]

        try:
            dssp = DSSP(model, str(cif_path), dssp='mkdssp')
        except Exception:
            dssp = DSSP(model, str(cif_path), dssp='dssp')

        ss_dict = {}
        for key, value in dssp.property_dict.items():
            ch, res_id = key
            if ch != chain_id:
                continue
            ss_code = value[2] or 'C'
            label_seq_id = res_id[1]
            ss_dict[label_seq_id] = ss_code

        return ss_dict

    except Exception as e:
        print(f"  âš ï¸ DSSPå¤±è´¥: {e}")
        return {}


# Helper: æ£€æµ‹DSSPç¼–å·ä½“ç³»
def _detect_dssp_numbering(dssp_dict, auth_to_label):
    """
    æ£€æµ‹DSSPé”®ä½¿ç”¨çš„æ˜¯authorç¼–å·è¿˜æ˜¯labelç¼–å·ã€‚
    è¿”å› 'author' æˆ– 'label'ã€‚
    é€»è¾‘ï¼šä¸auth_to_labelçš„key(autor)ä¸value(label)åˆ†åˆ«åšäº¤é›†ï¼Œè°å¤§ç”¨è°ã€‚
    """
    if not dssp_dict:
        return 'label'
    dssp_keys = set(dssp_dict.keys())
    author_ids = set(auth_to_label.keys())
    label_ids = set(auth_to_label.values())
    inter_author = len(dssp_keys & author_ids)
    inter_label = len(dssp_keys & label_ids)
    # å¹³æ‰‹æ—¶æ›´å¸¸è§çš„æ˜¯DSSPè¿”å›authorç¼–å·
    if inter_author >= inter_label:
        return 'author'
    return 'label'


def map_ss_to_uniprot_fixed(uniprot_seq, pdb_id, uniprot_id,
                             sifts_json=None, cache_dir="pdb_cache"):
    """
    ä¿®å¤ç‰ˆï¼šæ­£ç¡®å¤„ç†authorç¼–å·å’Œlabelç¼–å·çš„è½¬æ¢
    """
    print(f"\n{'='*70}")
    print(f"å¼€å§‹æ˜ å°„ {pdb_id} â†’ UniProt {uniprot_id}")
    print(f"{'='*70}")

    # ä¸‹è½½PDBæ–‡ä»¶
    cif_path = download_mmcif(pdb_id, cache_dir)
    if not cif_path:
        print(f"âŒ æ— æ³•ä¸‹è½½PDB {pdb_id}")
        return None
    print(f"âœ“ PDBæ–‡ä»¶å·²ä¸‹è½½")

    # è·å–SIFTSæ˜ å°„
    if sifts_json is None:
        print(f"  â†’ è·å–SIFTSæ˜ å°„...")
        sifts_json = fetch_sifts_mapping(pdb_id)

    if not sifts_json:
        print(f"âŒ æ— æ³•è·å–SIFTSæ˜ å°„")
        return None

    # SIFTSçš„æ®‹åŸºæ˜ å°„ï¼ˆauthorç¼–å· -> UniProtä½ç½®ï¼‰
    residue_map = build_residue_mapping_from_sifts(sifts_json, pdb_id, uniprot_id)

    if not residue_map:
        print(f"âŒ æœªèƒ½æ„å»ºæ®‹åŸºæ˜ å°„")
        return None

    print(f"âœ“ SIFTSæ˜ å°„äº† {len(residue_map)} ä¸ªæ®‹åŸº")

    # åˆå§‹åŒ–
    hec_array = ['C'] * len(uniprot_seq)
    mask_array = ['0'] * len(uniprot_seq)  # 1=æœ‰DSSPçœŸå€¼, 0=æ— çœŸå€¼(æœªè¦†ç›–)
    chains = set(chain for chain, _ in residue_map.keys())
    print(f"âœ“ å‘ç° {len(chains)} æ¡é“¾: {', '.join(sorted(chains))}")

    mapped_count = 0
    for chain_id in sorted(chains):
        print(f"\n{'-'*70}")
        print(f"å¤„ç†é“¾ {chain_id}")
        print(f"{'-'*70}")

        # 1. è·å–è¯¥é“¾çš„SIFTSæ˜ å°„ï¼ˆauthorç¼–å· -> UniProtï¼‰
        chain_sifts = {pdb_res: unp_idx for (ch, pdb_res), unp_idx in residue_map.items()
                       if ch == chain_id}
        sifts_nums = sorted(chain_sifts.keys())
        print(f"  [SIFTS] {len(chain_sifts)} ä¸ªæ®‹åŸº")
        print(f"          Authorç¼–å·èŒƒå›´: {min(sifts_nums)} - {max(sifts_nums)}")
        print(f"          å‰5ä¸ª: {sifts_nums[:5]}")

        # 2. è¯»å–author->labelç¼–å·æ˜ å°„
        auth_to_label = get_residue_numbering_from_mmcif(cif_path, chain_id)
        if not auth_to_label:
            print(f"  [æ˜ å°„] âš ï¸ æ— æ³•è·å–ç¼–å·æ˜ å°„ï¼Œå‡è®¾author=label")
            auth_to_label = {k: k for k in chain_sifts.keys()}
        else:
            print(f"  [æ˜ å°„] {len(auth_to_label)} ä¸ªæ®‹åŸºçš„authorâ†’labelæ˜ å°„")
            # æ˜¾ç¤ºå‡ ä¸ªæ˜ å°„ç¤ºä¾‹
            sample_keys = sorted(auth_to_label.keys())[:5]
            print(f"          ç¤ºä¾‹: ", end="")
            for k in sample_keys:
                print(f"{k}â†’{auth_to_label[k]}", end=" ")
            print()

        # 3. è¿è¡ŒDSSPï¼ˆè·å¾—labelç¼–å·çš„ç»“æœï¼‰
        dssp_dict = run_dssp(cif_path, chain_id)
        if not dssp_dict:
            print(f"  [DSSP] âŒ å¤±è´¥")
            continue
        dssp_nums = sorted(dssp_dict.keys())
        print(f"  [DSSP] {len(dssp_dict)} ä¸ªæ®‹åŸº")
        print(f"         Labelç¼–å·èŒƒå›´: {min(dssp_nums)} - {max(dssp_nums)}")
        print(f"         å‰5ä¸ª: {dssp_nums[:5]}")
        # 2.5 æ£€æµ‹DSSPç¼–å·ä½“ç³»ï¼ˆauthor vs labelï¼‰
        numbering_mode = _detect_dssp_numbering(dssp_dict, auth_to_label)
        print(f"  [ç¼–å·] æ£€æµ‹åˆ°DSSPä½¿ç”¨: {numbering_mode} ç¼–å·")

        # 4. è½¬æ¢å¹¶æ˜ å°„
        chain_mapped = 0
        mismatches = []

        for auth_num, unp_idx in chain_sifts.items():
            # author -> label
            label_num = auth_to_label.get(auth_num)

            if numbering_mode == 'author':
                dssp_key = auth_num
            else:
                if label_num is None:
                    mismatches.append(f"author={auth_num}æ— labelæ˜ å°„")
                    continue
                dssp_key = label_num

            # å–DSSPçš„8æ€ï¼Œè½¬æ¢åˆ°HECï¼Œå†å†™å›åˆ°å¯¹åº”UniProtç´¢å¼•
            if dssp_key in dssp_dict:
                if 0 <= unp_idx < len(uniprot_seq):
                    ss8 = dssp_dict[dssp_key]
                    hec_array[unp_idx] = ss8_to_hec(ss8)
                    mask_array[unp_idx] = '1'
                    chain_mapped += 1
                else:
                    mismatches.append(f"UniProtç´¢å¼•{unp_idx}è¶Šç•Œ")
            else:
                mismatches.append(f"DSSPæ— è¯¥ç¼–å·({numbering_mode}={dssp_key})")

        print(f"\n  [ç»“æœ] âœ“ æˆåŠŸæ˜ å°„: {chain_mapped}/{len(chain_sifts)} ä¸ªæ®‹åŸº ({chain_mapped/len(chain_sifts)*100:.1f}%)")

        if mismatches and len(mismatches) <= 10:
            print(f"  [é—®é¢˜] æœªæ˜ å°„çš„æ®‹åŸº: {', '.join(mismatches[:10])}")
        elif len(mismatches) > 10:
            print(f"  [é—®é¢˜] {len(mismatches)} ä¸ªæ®‹åŸºæœªæ˜ å°„ (ä»…æ˜¾ç¤ºå‰10ä¸ª): {', '.join(mismatches[:10])}")

        mapped_count += chain_mapped

    # ç»Ÿè®¡
    coverage = sum(1 for c in hec_array if c != 'C')
    coverage_pct = coverage / len(uniprot_seq) * 100

    h_count = sum(1 for c in hec_array if c == 'H')
    e_count = sum(1 for c in hec_array if c == 'E')
    c_count = sum(1 for c in hec_array if c == 'C')

    print(f"\n{'='*70}")
    print(f"æœ€ç»ˆç»Ÿè®¡")
    print(f"{'='*70}")
    print(f"  UniProtåºåˆ—é•¿åº¦: {len(uniprot_seq)} aa")
    print(f"  æ€»æ˜ å°„æ®‹åŸºæ•°: {mapped_count}")
    print(f"  ç»“æ„è¦†ç›–: {coverage}/{len(uniprot_seq)} ({coverage_pct:.1f}%)")
    print(f"  äºŒçº§ç»“æ„åˆ†å¸ƒ:")
    print(f"    H (èºæ—‹): {h_count} ({h_count/len(uniprot_seq)*100:.1f}%)")
    print(f"    E (æŠ˜å ): {e_count} ({e_count/len(uniprot_seq)*100:.1f}%)")
    print(f"    C (æ— è§„): {c_count} ({c_count/len(uniprot_seq)*100:.1f}%)")

    return ''.join(hec_array), ''.join(mask_array)

# ==================== ç¬¬5æ­¥: ä¸»æµç¨‹ ====================
def generate_ss_dataset(query="hemoglobin alpha",
                       max_proteins=100,
                       output_csv="ss_dataset.csv"):
    """
    å®Œæ•´æµç¨‹ï¼šç”ŸæˆäºŒçº§ç»“æ„æ•°æ®é›†
    """
    print("="*60)
    print("ğŸ§¬ äºŒçº§ç»“æ„æ•°æ®é›†ç”Ÿæˆå™¨")
    print("="*60)

    # Step 1: æœç´¢UniProt
    print(f"\nğŸ“ æ­¥éª¤1: æœç´¢UniProt (query='{query}')")
    uniprot_ids = uniprot_search_ids(query, size=max_proteins)
    print(f"   æ‰¾åˆ° {len(uniprot_ids)} ä¸ªUniProt ID")

    if not uniprot_ids:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•UniProt IDï¼Œé€€å‡º")
        return

    # Step 2-5: é€ä¸ªå¤„ç†
    results = []
    failed_count = 0

    for i, uid in enumerate(tqdm(uniprot_ids, desc="å¤„ç†ä¸­"), 1):
        try:
            # è·å–åºåˆ—
            seq = fetch_uniprot_fasta(uid)
            if not seq:
                failed_count += 1
                continue

            # æ‰¾æœ€ä½³PDB
            pdb_id = choose_best_pdb(uid)
            if not pdb_id:
                failed_count += 1
                continue

            # è·å–SIFTSæ˜ å°„
            sifts = fetch_sifts_mapping(pdb_id)
            mappings = extract_chain_mappings(sifts, pdb_id, uid)
            if not mappings:
                failed_count += 1
                continue

            # DSSPæ ‡æ³¨
            hec, mask = map_ss_to_uniprot_fixed(seq, pdb_id, uid)
            if not hec or len(hec) != len(seq) or len(mask) != len(seq):
                failed_count += 1
                continue

            # ä¿å­˜ç»“æœ
            results.append({
                'uniprot_id': uid,
                'pdb_id': pdb_id,
                'sequence': seq,
                'ss': hec,
                'length': len(seq),
                'mask': mask,
                'coverage': mask.count('1') / len(seq)
            })

            # é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(0.5)

        except Exception as e:
            print(f"\n  âŒ {uid} å¤„ç†å¤±è´¥: {e}")
            failed_count += 1
            continue

    # ä¿å­˜CSV
    if results:
        df = pd.DataFrame(results)
        # æ„å»ºæŒ‰æ®‹åŸºå±•å¼€çš„é•¿è¡¨ï¼ˆtoken-levelï¼‰ï¼Œä»…åŒ…å«æœ‰DSSPçœŸå€¼çš„ä½ç½®
        long_rows = []
        for row in results:
            uid = row['uniprot_id']
            pdb = row['pdb_id']
            seq = row['sequence']
            ss  = row['ss']
            mask = row['mask']
            for i, m in enumerate(mask):
                if m == '1':
                    long_rows.append({
                        'uniprot_id': uid,
                        'pdb_id': pdb,
                        'position': i + 1,
                        'aa': seq[i],
                        'label_hec': ss[i]
                    })
        long_df = pd.DataFrame(long_rows)
        df.to_csv(output_csv, index=False)
        long_csv = output_csv.replace('.csv', '.residues.csv')
        long_df.to_csv(long_csv, index=False)

        print("\n" + "="*60)
        print("âœ… æ•°æ®é›†ç”Ÿæˆå®Œæˆï¼")
        print("="*60)
        print(f"ğŸ“Š æˆåŠŸ: {len(results)} æ¡")
        print(f"âŒ å¤±è´¥: {failed_count} æ¡")
        print(f"ğŸ’¾ ä¿å­˜åˆ°: {output_csv}")
        print(f"ğŸ§ª æ®‹åŸºçº§CSV: {long_csv}")
        print("\næ•°æ®é¢„è§ˆ:")
        print(df[['uniprot_id', 'pdb_id', 'length']].head())

        # ç»Ÿè®¡äºŒçº§ç»“æ„åˆ†å¸ƒ
        all_ss = ''.join(df['ss'])
        h_pct = all_ss.count('H') / len(all_ss) * 100
        e_pct = all_ss.count('E') / len(all_ss) * 100
        c_pct = all_ss.count('C') / len(all_ss) * 100
        print(f"\näºŒçº§ç»“æ„åˆ†å¸ƒ:")
        print(f"  H (Helix): {h_pct:.1f}%")
        print(f"  E (Sheet): {e_pct:.1f}%")
        print(f"  C (Coil):  {c_pct:.1f}%")

        return df
    else:
        print("\nâŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ•°æ®ï¼")
        return None


def generate_multiquery_dataset(queries,
                                max_proteins_per_query=30,
                                output_prefix="ss_multi_dataset.csv"):
    """
    å¤šå…³é”®è¯æ”¶é›†å™¨ï¼šä¾æ¬¡æœç´¢å¤šä¸ª queryï¼Œåˆå¹¶å»é‡ï¼Œç”Ÿæˆä¸¤ä»½ CSV
    - queries: list[str]
    - max_proteins_per_query: æ¯ä¸ª query æŠ“å–çš„ UniProt ä¸Šé™
    - output_prefix: ä¸»CSVæ–‡ä»¶åï¼›æ®‹åŸºçº§CSVä¼šè‡ªåŠ¨ç”¨ .residues.csv
    è¯´æ˜ï¼š
    * ä¸ºäº†æé«˜æˆåŠŸç‡ï¼Œè¿™é‡Œä½¿ç”¨ choose_best_pdb_with_validationï¼ˆå¸¦SIFTSè¦†ç›–æ£€æŸ¥ï¼‰
    * æ¯æ¡è®°å½•ä¼šå¤šä¸€ä¸ª 'source_query' å­—æ®µï¼Œæ ‡æ³¨æ¥è‡ªå“ªä¸ªå…³é”®è¯
    """
    print("="*60)
    print("ğŸ§¬ å¤šå…³é”®è¯äºŒçº§ç»“æ„æ•°æ®é›†ç”Ÿæˆå™¨")
    print("="*60)

    results = []
    failed = 0
    seen_uids = set()  # è·¨queryå»é‡

    for q_idx, query in enumerate(queries, 1):
        print(f"\nğŸ“ Query[{q_idx}/{len(queries)}]: '{query}'")
        uids = uniprot_search_ids(query, size=max_proteins_per_query)
        print(f"   æ‰¾åˆ° {len(uids)} ä¸ªUniProt ID (é™åˆ¶ {max_proteins_per_query})")

        for uid in tqdm(uids, desc=f"å¤„ç†ä¸­[{query}]"):
            if uid in seen_uids:
                continue
            try:
                seq = fetch_uniprot_fasta(uid)
                if not seq:
                    failed += 1
                    continue

                # è¿‡é•¿åºåˆ—ç›´æ¥è·³è¿‡ï¼ˆè¶…å¤§å¤åˆç‰©/ä½åˆ†è¾¨ç‡ç»“æ„å¸¸è§ï¼‰
                if len(seq) > 800:
                    print(f"  âš ï¸ åºåˆ—è¿‡é•¿({len(seq)} aa) â€” è·³è¿‡")
                    failed += 1
                    continue

                # ä½¿ç”¨å¸¦è¦†ç›–éªŒè¯çš„PDBé€‰æ‹©å™¨
                pdb_id, reso, spans = choose_best_pdb_with_validation(uid, max_candidates=20)
                if not pdb_id:
                    failed += 1
                    continue

                sifts = fetch_sifts_mapping(pdb_id)
                mappings = extract_chain_mappings(sifts, pdb_id, uid)
                if not mappings:
                    failed += 1
                    continue

                # ä¼°ç®—è¦†ç›–æ¯”ä¾‹ï¼ˆæŒ‰UniProtåŒºé—´ï¼‰
                est_cov = 0
                for (_chain, unp_start, unp_end, _pdb_start, _pdb_end) in mappings:
                    if unp_start is not None and unp_end is not None:
                        est_cov += (unp_end - unp_start + 1)
                cov_ratio = est_cov / len(seq)
                if cov_ratio < 0.4:
                    print(f"  âš ï¸ è¦†ç›–åä½({cov_ratio:.2f}) â€” è·³è¿‡")
                    failed += 1
                    continue

                hec, mask = map_ss_to_uniprot_fixed(seq, pdb_id, uid)
                if not hec or len(hec) != len(seq) or len(mask) != len(seq):
                    failed += 1
                    continue

                results.append({
                    'uniprot_id': uid,
                    'pdb_id': pdb_id,
                    'sequence': seq,
                    'ss': hec,
                    'length': len(seq),
                    'mask': mask,
                    'coverage': mask.count('1') / len(seq),
                    'source_query': query
                })
                seen_uids.add(uid)

                time.sleep(0.8)  # è½»é™æµ

            except Exception as e:
                print(f"\n  âŒ {uid} å¤„ç†å¤±è´¥: {e}")
                failed += 1
                continue

    if not results:
        print("\nâŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ•°æ®ï¼")
        return None

    # æ±‡æ€»ä¸ä¿å­˜
    import pandas as pd
    df = pd.DataFrame(results)

    # æ®‹åŸºçº§é•¿è¡¨
    long_rows = []
    for row in results:
        uid = row['uniprot_id']
        pdb = row['pdb_id']
        seq = row['sequence']
        ss  = row['ss']
        mask = row['mask']
        src = row['source_query']
        for i, m in enumerate(mask):
            if m == '1':
                long_rows.append({
                    'uniprot_id': uid,
                    'pdb_id': pdb,
                    'position': i + 1,
                    'aa': seq[i],
                    'label_hec': ss[i],
                    'source_query': src
                })
    long_df = pd.DataFrame(long_rows)

    # ä¿å­˜
    main_csv = output_prefix
    res_csv  = output_prefix.replace('.csv', '.residues.csv')
    df.to_csv(main_csv, index=False)
    long_df.to_csv(res_csv, index=False)

    print("\n" + "="*60)
    print("âœ… å¤šå…³é”®è¯æ•°æ®é›†ç”Ÿæˆå®Œæˆï¼")
    print("="*60)
    print(f"ğŸ“Š æˆåŠŸ: {len(results)} æ¡ | å¤±è´¥: {failed} æ¡ | å»é‡åUID: {len(seen_uids)}")
    print(f"ğŸ’¾ ä¸»CSV: {main_csv}")
    print(f"ğŸ§ª æ®‹åŸºçº§CSV: {res_csv}")

    # ç®€è¦åˆ†å¸ƒ
    all_ss = ''.join(df['ss'])
    h_pct = all_ss.count('H') / len(all_ss) * 100
    e_pct = all_ss.count('E') / len(all_ss) * 100
    c_pct = all_ss.count('C') / len(all_ss) * 100
    print(f"\näºŒçº§ç»“æ„åˆ†å¸ƒ: H {h_pct:.1f}% | E {e_pct:.1f}% | C {c_pct:.1f}%")

    print("\næ¥æºæ¦‚è§ˆï¼ˆæ¯ä¸ªå…³é”®è¯æ”¶é›†åˆ°çš„æ¡ç›®æ•°ï¼‰:")
    print(df['source_query'].value_counts())

    return df

# ==================== Colabå…¥å£ ====================
if __name__ == "__main__":
    # å¯åœ¨æ­¤è°ƒæ•´ï¼šmax_proteins_per_queryã€å€™é€‰PDBä¸Šé™åœ¨ choose_best_pdb_with_validation(max_candidates=20)ã€é•¿åº¦é˜ˆå€¼(>800è·³è¿‡)ã€è¦†ç›–é˜ˆå€¼( <0.4 è·³è¿‡ )
    # åœ¨Colabä¸­è¿è¡Œå‰å…ˆå®‰è£…ä¾èµ–
    print("æ£€æŸ¥ä¾èµ–...")
    try:
        from Bio.PDB.DSSP import DSSP
        print("âœ… Biopythonå·²å®‰è£…")
    except Exception:
        print("âš ï¸ æ­£åœ¨å®‰è£…Biopython...")
        import subprocess
        subprocess.check_call(['pip', 'install', '-q', 'biopython'])

    # æ£€æŸ¥DSSP
    import subprocess
    try:
        subprocess.run(['mkdssp', '--version'], capture_output=True, check=True)
        print("âœ… DSSPå·²å®‰è£…")
    except Exception:
        print("âš ï¸ æ­£åœ¨å®‰è£…DSSP...")
        subprocess.check_call(['apt-get', 'install', '-y', '-qq', 'dssp'])

    print("\nå¼€å§‹ç”Ÿæˆæ•°æ®é›†...\n")

    # å¤šå…³é”®è¯ç¤ºä¾‹ï¼ˆå¯åœ¨æ­¤ç¼–è¾‘å…³é”®è¯åˆ—è¡¨ï¼‰
    queries = [
        # å…¨Î±
        "hemoglobin",
        "myoglobin",
        # é«˜Î²
        "green fluorescent protein",
        "porin",
        "immunoglobulin domain",
        "beta propeller",
        # Î±/Î²
        "triosephosphate isomerase",
        "enolase",
        "aldolase",
    ]
    df = generate_multiquery_dataset(
        queries=queries,
        max_proteins_per_query=50,
        output_prefix="ss_multi_dataset.csv"
    )

    # æ˜¾ç¤ºç¤ºä¾‹
    if df is not None:
        print("\nç¤ºä¾‹æ•°æ®:")
        for idx, row in df.head(3).iterrows():
            print(f"\nåºåˆ— {idx+1}:")
            print(f"  UniProt: {row['uniprot_id']}")
            print(f"  PDB: {row['pdb_id']}")
            print(f"  é•¿åº¦: {row['length']}")
            print(f"  åºåˆ—: {row['sequence'][:50]}...")
            print(f"  ç»“æ„: {row['ss'][:50]}...")
